{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
        "!unzip -qq Flickr8k_Dataset.zip\n",
        "!unzip -qq Flickr8k_text.zip\n",
        "!rm Flickr8k_Dataset.zip Flickr8k_text.zip"
      ],
      "metadata": {
        "id": "IHIbuqTmRyG_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Attention\n",
        "from tensorflow.keras.layers import Add, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.applications.xception import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "import numpy as np\n",
        "from pickle import load\n",
        "import os\n",
        "import nltk\n",
        "import string\n",
        "from PIL import Image\n",
        "# Set the seed for reproducibility\n",
        "tf.random.set_seed(42)\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mup27xhdRz89",
        "outputId": "daff6785-5f8c-442d-c3d7-b11ec81c1a21"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading a text file into memory\n",
        "def load_doc(filename):\n",
        "    # Opening the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# get all imgs with their captions\n",
        "def all_img_captions(filename):\n",
        "    file = load_doc(filename)\n",
        "    captions = file.split('\\n')\n",
        "    descriptions = {}\n",
        "    for caption in captions[:-1]:\n",
        "        img, caption = caption.split('\\t')\n",
        "        if img[:-2] not in descriptions:\n",
        "            descriptions[img[:-2]] = [caption]\n",
        "        else:\n",
        "            descriptions[img[:-2]].append(caption)\n",
        "    return descriptions\n",
        "\n",
        "# Data cleaning - lowercasing, removing punctuation, and words containing numbers\n",
        "def cleaning_text(captions):\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    for img, caps in captions.items():\n",
        "        for i, img_caption in enumerate(caps):\n",
        "\n",
        "            img_caption.replace(\"-\", \" \")\n",
        "            desc = img_caption.split()\n",
        "\n",
        "            # Converts to lowercase\n",
        "            desc = [word.lower() for word in desc]\n",
        "            # Remove punctuation from each token\n",
        "            desc = [word.translate(table) for word in desc]\n",
        "            # Remove hanging 's' and 'a'\n",
        "            desc = [word for word in desc if (len(word) > 1)]\n",
        "            # Remove tokens with numbers in them\n",
        "            desc = [word for word in desc if (word.isalpha())]\n",
        "            # Convert back to string\n",
        "\n",
        "            img_caption = ' '.join(desc)\n",
        "            captions[img][i] = img_caption\n",
        "    return captions\n",
        "\n",
        "# Building vocabulary\n",
        "def text_vocabulary(descriptions):\n",
        "    # Build vocabulary of all unique words\n",
        "    vocab = set()\n",
        "\n",
        "    for key in descriptions.keys():\n",
        "        [vocab.update(d.split()) for d in descriptions[key]]\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# All descriptions in one file\n",
        "def save_descriptions(descriptions, filename):\n",
        "    lines = list()\n",
        "    for key, desc_list in descriptions.items():\n",
        "        for desc in desc_list:\n",
        "            lines.append(key + '\\t' + desc)\n",
        "    data = \"\\n\".join(lines)\n",
        "    file = open(filename, \"w\")\n",
        "    file.write(data)\n",
        "    file.close()"
      ],
      "metadata": {
        "id": "28fza8HGRz6J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set these paths according to your project folder in your system\n",
        "dataset_text = \"/content\"\n",
        "dataset_images = \"/content/Flicker8k_Dataset\"\n",
        "\n",
        "# We prepare our text data\n",
        "filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n",
        "# Loading the file that contains all data\n",
        "# Mapping them into descriptions dictionary img to 5 captions\n",
        "descriptions = all_img_captions(filename)\n",
        "print(\"Length of descriptions =\", len(descriptions))\n",
        "\n",
        "# Cleaning the descriptions\n",
        "clean_descriptions = cleaning_text(descriptions)\n",
        "\n",
        "# Building vocabulary\n",
        "vocabulary = text_vocabulary(clean_descriptions)\n",
        "print(\"Length of vocabulary = \", len(vocabulary))\n",
        "\n",
        "# Saving each description to a file\n",
        "save_descriptions(clean_descriptions, \"descriptions.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHgbTK13RzwE",
        "outputId": "922c7f60-320a-4205-d4a6-cc672ab824b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of descriptions = 8092\n",
            "Length of vocabulary =  8763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_features(directory):\n",
        "#     model = Xception(include_top=False, pooling='avg')\n",
        "#     features = {}\n",
        "#     for img in tqdm(os.listdir(directory)):\n",
        "#         filename = directory + \"/\" + img\n",
        "#         image = Image.open(filename)\n",
        "#         image = image.resize((299, 299))\n",
        "#         image = np.expand_dims(image, axis=0)\n",
        "#         image = image / 127.5\n",
        "#         image = image - 1.0\n",
        "\n",
        "#         feature = model.predict(image)\n",
        "#         features[img] = feature\n",
        "#     return features\n",
        "\n",
        "# # 2048 feature vector\n",
        "# features = extract_features(dataset_images)\n",
        "# dump(features, open(\"features.p\", \"wb\"))"
      ],
      "metadata": {
        "id": "pUqFme07Rzt3"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "def load_photos(filename):\n",
        "    file = load_doc(filename)\n",
        "    photos = file.split(\"\\n\")[:-1]\n",
        "    return photos\n",
        "\n",
        "def load_clean_descriptions(filename, photos):\n",
        "    # Loading clean_descriptions\n",
        "    file = load_doc(filename)\n",
        "    descriptions = {}\n",
        "    for line in file.split(\"\\n\"):\n",
        "\n",
        "        words = line.split()\n",
        "        if len(words) < 1:\n",
        "            continue\n",
        "\n",
        "        image, image_caption = words[0], words[1:]\n",
        "\n",
        "        if image in photos:\n",
        "            if image not in descriptions:\n",
        "                descriptions[image] = []\n",
        "            desc = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
        "            descriptions[image].append(desc)\n",
        "\n",
        "    return descriptions\n",
        "\n",
        "def load_features(photos):\n",
        "    # Loading all features\n",
        "    all_features = load(open(\"features.p\", \"rb\"))\n",
        "    # Selecting only needed features\n",
        "    features = {k: all_features[k] for k in photos}\n",
        "    return features\n",
        ""
      ],
      "metadata": {
        "id": "mK0sTYd0RzrN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
        "# train = loading_data(filename)\n",
        "train_imgs = load_photos(filename)\n",
        "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
        "train_features = load_features(train_imgs)"
      ],
      "metadata": {
        "id": "6B4cjl6nRzo3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting dictionary to a clean list of descriptions\n",
        "def dict_to_list(descriptions):\n",
        "    all_desc = []\n",
        "    for key in descriptions.keys():\n",
        "        [all_desc.append(d) for d in descriptions[key]]\n",
        "    return all_desc\n",
        "\n",
        "# Creating a tokenizer class\n",
        "# This will vectorize the text corpus\n",
        "# Each integer will represent a token in the dictionary\n",
        "\n",
        "def create_tokenizer(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(desc_list)\n",
        "    return tokenizer\n",
        "\n",
        "# Give each word an index and store that into tokenizer.p pickle file\n",
        "tokenizer = create_tokenizer(train_descriptions)\n",
        "#dump(tokenizer, open('tokenizer.p', 'wb'))\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "vocab_size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAxaC04qRzmx",
        "outputId": "91291a1d-a18f-49d6-b64b-3bf9e848b49b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7577"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the maximum length of descriptions\n",
        "def max_length(descriptions):\n",
        "    desc_list = dict_to_list(descriptions)\n",
        "    return max(len(d.split()) for d in desc_list)\n",
        "\n",
        "max_length = max_length(descriptions)\n",
        "max_length\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q1zChd0Rzj7",
        "outputId": "6f9e7a0f-5811-4387-b4e1-0417cbc03c30"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Max_lengths is 32, vocab_size is 7577\n",
        "\n",
        "# Create input-output sequence pairs from the image description\n",
        "\n",
        "# Data generator, used by model.fit_generator()\n",
        "def data_generator(descriptions, features, tokenizer, max_length):\n",
        "    while 1:\n",
        "        for key, description_list in descriptions.items():\n",
        "            # Retrieve photo features\n",
        "            feature = features[key][0]\n",
        "            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
        "            yield [[input_image, input_sequence], output_word]\n",
        "\n",
        "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    # Get the maximum sequence length\n",
        "    max_seq_length = max_length\n",
        "\n",
        "    # Walk through each description for the image\n",
        "    for desc in desc_list:\n",
        "        # Encode the sequence\n",
        "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
        "\n",
        "        # Split one sequence into multiple X, y pairs\n",
        "        for i in range(1, len(seq)):\n",
        "            # Split into input and output pair\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "\n",
        "            # Pad input sequence if it's shorter than max_seq_length\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_seq_length)[0]\n",
        "\n",
        "            # Encode output sequence\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "\n",
        "            # Store\n",
        "            X1.append(feature)\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "\n"
      ],
      "metadata": {
        "id": "1SEfVj52Rzhr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Concatenate\n",
        "\n",
        "def define_model_with_attention(vocab_size, max_length):\n",
        "    # Image feature extractor\n",
        "    input_image_features = tf.keras.layers.Input(shape=(2048,))\n",
        "    image_features = Dense(256, activation='relu')(input_image_features)\n",
        "\n",
        "    # Sequence model\n",
        "    input_sequence = tf.keras.layers.Input(shape=(max_length,))\n",
        "    sequence_embedding = Embedding(vocab_size, 256, mask_zero=True)(input_sequence)\n",
        "    sequence_lstm = LSTM(256, return_sequences=True)(sequence_embedding)\n",
        "\n",
        "    # Attention mechanism\n",
        "    attention = tf.keras.layers.Attention()([sequence_lstm, image_features])\n",
        "    context = Concatenate(axis=-1)([sequence_lstm, attention])\n",
        "\n",
        "    # Decoder\n",
        "    decoder_lstm1 = LSTM(256)(context)\n",
        "    output = Dense(vocab_size, activation='softmax')(decoder_lstm1)\n",
        "\n",
        "    # Combine the inputs and outputs into a complete model\n",
        "    model = tf.keras.models.Model(inputs=[input_image_features, input_sequence], outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define your vocabulary size and maximum sequence length\n",
        "vocab_size = 7577\n",
        "max_length = 32\n",
        "\n",
        "# Create the model\n",
        "model = define_model_with_attention(vocab_size, max_length)\n",
        "\n",
        "# Compile the model and specify your loss and optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY9pE9_uRzfE",
        "outputId": "f6f5a439-bb9d-4262-90fb-76f7083d4544"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 32)]                 0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 32, 256)              1939712   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)        [(None, 2048)]               0         []                            \n",
            "                                                                                                  \n",
            " lstm (LSTM)                 (None, 32, 256)              525312    ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 256)                  524544    ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " attention (Attention)       (None, 32, 256)              0         ['lstm[0][0]',                \n",
            "                                                                     'dense[0][0]']               \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 32, 512)              0         ['lstm[0][0]',                \n",
            "                                                                     'attention[0][0]']           \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)               (None, 256)                  787456    ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 7577)                 1947289   ['lstm_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 5724313 (21.84 MB)\n",
            "Trainable params: 5724313 (21.84 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "print('Dataset: ', len(train_imgs))\n",
        "print('Descriptions: train=', len(train_descriptions))\n",
        "print('Photos: train=', len(train_features))\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "print('Description Length: ', max_length)\n",
        "\n",
        "epochs = 30\n",
        "steps = len(train_descriptions)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzduB2mhRzcp",
        "outputId": "3bdd1890-fa12-442c-b823-2bcf84e85c12"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset:  6000\n",
            "Descriptions: train= 6000\n",
            "Photos: train= 6000\n",
            "Vocabulary Size: 7577\n",
            "Description Length:  32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir(\"models\")"
      ],
      "metadata": {
        "id": "nRwaehuCRzag"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a directory to save our models\n",
        "\n",
        "for i in range(epochs):\n",
        "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
        "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "    model.save(\"models/model_\" + str(i) + \".h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7aimRggRzYg",
        "outputId": "ff326680-db7f-41e0-90f4-542cc17d83b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-22-a424b933afa8>:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6000/6000 [==============================] - 1143s 188ms/step - loss: 4.5691\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6000/6000 [==============================] - 1101s 183ms/step - loss: 3.6064\n",
            "6000/6000 [==============================] - 1117s 186ms/step - loss: 3.2313\n",
            "6000/6000 [==============================] - 1118s 186ms/step - loss: 2.9681\n",
            "6000/6000 [==============================] - 1105s 184ms/step - loss: 2.7660\n",
            "6000/6000 [==============================] - 1086s 181ms/step - loss: 2.5999\n",
            "6000/6000 [==============================] - 1064s 177ms/step - loss: 2.4613\n",
            "6000/6000 [==============================] - 1064s 177ms/step - loss: 2.3405\n",
            "6000/6000 [==============================] - 1062s 177ms/step - loss: 2.2347\n",
            "6000/6000 [==============================] - 1068s 178ms/step - loss: 2.1386\n",
            "6000/6000 [==============================] - 1057s 176ms/step - loss: 2.0556\n",
            "2685/6000 [============>.................] - ETA: 9:43 - loss: 1.9996"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yyRxCIdGRzV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_eD_scvGRzTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CVU2gNQTRzQx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}